import os  #å¯¼å…¥pythonçš„æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class QWenGenerator:
    """
    æœ¬åœ° QWen æ¨¡å‹å°è£…æ¨¡å—
    ç›´æ¥ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
    """

    def __init__(self, local_model_path="D:/Models/Qwen1.5-1.8B"):
        """
        åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        
        Args:
            local_model_path: æœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“Œ å½“å‰ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ“ å°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {local_model_path}")

        try:
            # ä»æœ¬åœ°åŠ è½½ tokenizer
            print("â¬‡ï¸ åŠ è½½ tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                local_model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            
            # ä»æœ¬åœ°åŠ è½½æ¨¡å‹
            print("â¬‡ï¸ åŠ è½½æ¨¡å‹...")
            
            # è®¾ç½®åˆé€‚çš„dtype
            if self.device == "cuda":
                torch_dtype = torch.float16
                print("âœ… ä½¿ç”¨ float16 ç²¾åº¦ (GPU)")
            else:
                torch_dtype = torch.float32
                print("âœ… ä½¿ç”¨ float32 ç²¾åº¦ (CPU)")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                local_model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device == "cuda" else None,
                local_files_only=True
            )
            
            # å¦‚æœæ²¡æœ‰è‡ªåŠ¨åˆ†é…åˆ°è®¾å¤‡ï¼Œåˆ™æ‰‹åŠ¨åˆ†é…
            if self.device == "cuda" and self.model.device.type != "cuda":
                self.model = self.model.to(self.device)
                
            self.model.eval()
            print("âœ… QWen æ¨¡å‹åŠ è½½å®Œæˆï¼")
            
        except Exception as e:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
            print("1. ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ° D:/Models/Qwen1.5-1.8B/ ç›®å½•")
            print("2. ä¸‹è½½åœ°å€: https://huggingface.co/Qwen/Qwen1.5-1.8B")
            print("3. æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½:")
            print("   huggingface-cli download Qwen/Qwen1.5-1.8B --local-dir D:/Models/Qwen1.5-1.8B")
            print("\nğŸ’¡ æ³¨æ„: å¦‚æœç½‘ç»œæœ‰é—®é¢˜ï¼Œå¯ä»¥:")
            print("   - ä½¿ç”¨VPNæˆ–ä»£ç†")
            print("   - ä½¿ç”¨é•œåƒæº: è®¾ç½® HF_ENDPOINT=https://hf-mirror.com")
            print("   - æ‰‹åŠ¨ä»æµè§ˆå™¨ä¸‹è½½")
            raise

    def generate_greeting(self, prompt, max_new_tokens=200, temperature=0.7, top_p=0.9):
        """
        åŸºäºè¾“å…¥ prompt ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: è¾“å…¥æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰
            top_p: æ ¸é‡‡æ ·å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç è¾“å‡º
            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # åªè¿”å›ç”Ÿæˆéƒ¨åˆ†ï¼Œå»æ‰åŸå§‹prompt
            if text.startswith(prompt):
                return text[len(prompt):].strip()
            else:
                return text.strip()
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return "ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹é…ç½®ã€‚"