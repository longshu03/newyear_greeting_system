# æ–‡ä»¶åï¼štrain_lora.py
# ä½œç”¨ï¼šä½¿ç”¨ LoRA å¯¹ Qwen1.5-1.8B åš SFT æŒ‡ä»¤å¾®è°ƒ

import json
import torch
import os
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast, GradScaler  # ä¿®æ­£å¯¼å…¥

# é…ç½®
MODEL_PATH = "D:/Models/Qwen1.5-1.8B"   # ä½¿ç”¨æœ¬åœ°è·¯å¾„
DATA_PATH = "backend/LLM/finetune/sft_dataset.json"
OUTPUT_DIR = "backend/LLM/finetune/output"

MAX_LEN = 256  # å‡å°‘é•¿åº¦ä»¥èŠ‚çœæ˜¾å­˜
EPOCHS = 2     # å‡å°‘è½®æ•°
LR = 1e-5      # é€‚å½“é™ä½å­¦ä¹ ç‡
BATCH_SIZE = 1  # å°æ‰¹é‡å¤„ç†
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# åˆ›å»º LoRA é…ç½®
def get_lora_config():
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]  # å‡å°‘ç›®æ ‡æ¨¡å—ï¼Œåªé€‰æ‹©å…³é”®æ¨¡å—
    )

def load_dataset():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # æ•°æ®éªŒè¯
    print(f"ğŸ“Š åŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")
    for i, item in enumerate(data[:2]):  # é¢„è§ˆå‰2æ¡
        print(f"æ ·æœ¬ {i+1}:")
        print(f"  æŒ‡ä»¤: {item['instruction'][:50]}...")
        print(f"  è¾“å…¥: {item['input'][:50]}...")
        print(f"  è¾“å‡º: {item['output'][:50]}...")
        print()
    
    return data

def format_prompt(example):
    """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬"""
    return f"### æŒ‡ä»¤ï¼š\n{example['instruction']}\n\n### è¾“å…¥ï¼š\n{example['input']}\n\n### è¾“å‡ºï¼š\n{example['output']}"

def main():
    print("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ")
    print(f"ğŸ“Œ ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    
    # æ£€æŸ¥æ˜¾å­˜æƒ…å†µ
    if DEVICE == "cuda":
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        allocated_memory = torch.cuda.memory_allocated(0) / 1e9
        free_memory = total_memory - allocated_memory
        print(f"ğŸ’¾ GPUæ˜¾å­˜: æ€»å…± {total_memory:.2f}GB, å·²ç”¨ {allocated_memory:.2f}GB, å¯ç”¨ {free_memory:.2f}GB")
    
    # 1. åŠ è½½ tokenizer
    print("\nâ¬‡ï¸ åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        local_files_only=True  # ä»æœ¬åœ°åŠ è½½
    )
    
    # è®¾ç½®å¡«å……æ ‡è®°
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. åŠ è½½æ¨¡å‹
    print("â¬‡ï¸ åŠ è½½æ¨¡å‹...")
    try:
        # é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹
        if DEVICE == "cuda":
            # æ£€æŸ¥æ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ
            if free_memory > 3.0:  # å¦‚æœå¯ç”¨æ˜¾å­˜å¤§äº3GBï¼Œä½¿ç”¨float16
                torch_dtype = torch.float16
                print("âœ… ä½¿ç”¨ float16 ç²¾åº¦ (GPUæ˜¾å­˜å……è¶³)")
            else:
                torch_dtype = torch.float32
                print("âš ï¸  å¯ç”¨æ˜¾å­˜è¾ƒå°‘ï¼Œä½¿ç”¨ float32 ç²¾åº¦")
        else:
            torch_dtype = torch.float32
            print("âœ… ä½¿ç”¨ float32 ç²¾åº¦ (CPU)")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch_dtype,
            local_files_only=True
        )
        
        # å…³é”®ä¿®æ”¹ï¼šå°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(DEVICE)
        print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° {DEVICE}")
        
        # åº”ç”¨ LoRA
        print("ğŸ”§ åº”ç”¨ LoRA é…ç½®...")
        lora_config = get_lora_config()
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()  # æ‰“å°å¯è®­ç»ƒå‚æ•°æ•°é‡
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„åŠ è½½æ–¹å¼...")
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨CPUå’Œfloat32
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True,
            torch_dtype=torch.float32,
            local_files_only=True
        )
        model = model.to("cpu")
        DEVICE = "cpu"
        lora_config = get_lora_config()
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
    
    model.config.use_cache = False
    model.train()
    
    # 3. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    print("\nğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    dataset = load_dataset()
    
    # æ•°æ®é¢„å¤„ç†
    def tokenize_function(example):
        text = format_prompt(example)
        
        # Tokenize
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=MAX_LEN,
            padding="max_length",
            return_tensors="pt"
        )
        
        # åˆ›å»º labels
        input_ids = tokenized["input_ids"][0]
        attention_mask = tokenized["attention_mask"][0]
        
        # ç®€åŒ–ï¼šæ•´ä¸ªåºåˆ—éƒ½ä½œä¸º labels
        labels = input_ids.clone()
        labels[attention_mask == 0] = -100  # å¿½ç•¥å¡«å……ä½ç½®
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        }
    
    tokenized_data = [tokenize_function(d) for d in dataset]
    
    # åˆ›å»º DataLoader
    dataloader = DataLoader(
        tokenized_data,
        batch_size=BATCH_SIZE,
        shuffle=True
    )
    
    # 4. ä¼˜åŒ–å™¨
    optimizer = AdamW(
        model.parameters(),
        lr=LR,
        weight_decay=0.01
    )
    
    # 5. è®­ç»ƒå¾ªç¯ - ç®€åŒ–ç‰ˆï¼Œä¸ä½¿ç”¨æ··åˆç²¾åº¦
    print("\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(EPOCHS):
        print(f"\nğŸ“š Epoch {epoch + 1}/{EPOCHS}")
        model.train()
        total_loss = 0
        
        for step, batch in enumerate(dataloader):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(DEVICE) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            loss = outputs.loss
            
            # æ£€æŸ¥ NaN/Inf
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸  æ­¥éª¤ {step}: æ£€æµ‹åˆ° NaN/Inf æŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                optimizer.zero_grad()
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            # ä¼˜åŒ–å™¨æ­¥è¿›
            optimizer.step()
            optimizer.zero_grad()
            
            total_loss += loss.item()
            
            if step % 1 == 0:  # æ¯ä¸ªæ‰¹æ¬¡éƒ½æ‰“å°
                print(f"  æ­¥éª¤ {step}/{len(dataloader)} | æŸå¤±: {loss.item():.4f}")
        
        # æ¯ä¸ª epoch çš„å¹³å‡æŸå¤±
        avg_loss = total_loss / len(dataloader)
        print(f"âœ… Epoch {epoch + 1} å®Œæˆ | å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # æ¯ä¸ªepochåä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_dir = os.path.join(OUTPUT_DIR, f"checkpoint-epoch-{epoch+1}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        model.save_pretrained(checkpoint_dir)
        tokenizer.save_pretrained(checkpoint_dir)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜åˆ°: {checkpoint_dir}")
    
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {OUTPUT_DIR}")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜ LoRA æƒé‡
    model.save_pretrained(OUTPUT_DIR)
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    # ä¿å­˜å®Œæ•´é…ç½®
    config = {
        "base_model": MODEL_PATH,
        "lora_config": lora_config.to_dict(),
        "training_args": {
            "epochs": EPOCHS,
            "learning_rate": LR,
            "batch_size": BATCH_SIZE,
            "max_length": MAX_LEN,
            "device": DEVICE
        }
    }
    
    config_path = os.path.join(OUTPUT_DIR, "training_config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {OUTPUT_DIR}")

if __name__ == "__main__":
    main()